{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "534e3c1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-19T04:47:53.261673Z",
     "start_time": "2022-01-19T04:47:53.256204Z"
    }
   },
   "source": [
    "# <center>           Reddit: A Window Into Ancient Worlds     </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d5dd06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-19T04:28:54.164767Z",
     "start_time": "2022-01-19T04:28:54.154903Z"
    }
   },
   "source": [
    "<img src=\"../images/romevgreece.jpg\"\n",
    "     style=\"float: left; margin-right: 2px;width:600px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da6a8c1",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "For centuries, historians have done the bulk of their work in the field. Be it an archaeological excavation on a remote location or arduous sifting through old documents in city archives, historians have seldom made a revolutionary new discovery just by sitting in their offices and pondering the times past.\n",
    "\n",
    "That has all changed over the past few decades as more and more research has been published online - from a detailed list of pottery found during an archaeological dig, to a groundbreaking new research article shedding light on previously unknown aspects of life and times long ago, and even to those dusty old archival texts, they have all, slowly but surely, found their way on to the World Wide Web.\n",
    "\n",
    "With such a vast array of historical information at their fingertips, historians are no longer able to rely on simple browser search techniques to find the relevant content. As a result, a top university has hired us to build a machine learning model which can perform that search for them, both for history, as well as, down the road, for other academic departments.\n",
    "\n",
    "Why did we choose to start with history? There are several reason, but the main one is that...it's in the past. As such, the topics, discussions and words used are not changing as much as in other fields such as technology and medicine where disciplines seem to find an entirely knew paradigm from one year to the next. To be sure, progress in historical research still takes place, but instead of reinventing the entire field, it takes place at the margins. That is why it is perfectly suitable for a machine learning model - once a model learns the language used in a historical topic, that language will not change much from year to year, making the model efficient to use and simple to maintain for longer periods of time.\n",
    "\n",
    "\n",
    "## Task\n",
    "\n",
    "The first stage of teaching the machine to recognize topics is to train them to distinguish between only a pair of them. This is where this project comes in.\n",
    "\n",
    "We have chosen to train and test the model on Reddit website, namely, its two subreddits dedicated to Ancient Greece and Ancient Rome. Reddit, as an online public forum, has a wide variety of participants - from university professors to teenagers, and as such, represents a microcosm of online experience. Another benefit is that the two ancient civilizations are far removed from current events, pop culture and vitriolic debates that pervade online spaces, and yet, because of their importance in the development of our civilization, they still manage to have active subreddits with close to 50k followers each. Such a diverse, but relatively serious set of followers should produce less spam and more useful vocabulary for the machine to train on than most other subreddits.\n",
    "\n",
    "In this first step of the project we will develop a model that can learn to distinguish between texts topics on ancient Rome and Greece based on 6000 posts taken from Reddit. \n",
    "\n",
    "Further on down the line, the model will have to be trained to distinguish between many different topics all at once, as well as to recognize when a topic does not belong to any of the academic fields, but that is beyond the scope of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ecc93c",
   "metadata": {},
   "source": [
    "## Metrics and their names\n",
    "\n",
    "For the purposes of the project, our model evaluation will mostly be based on total accuracy - basically, the percentage of correct predictions. Since there is no real difference between mistaking topic number one for number two vs vice versa, there is no real point to have classification metrics of 1 for positive outcome and 0 for negative. Granted, we will still use 1 and 0, but only in purely nominal terms, with 1 representing greece and 0 rome.\n",
    "\n",
    "Concepts of false positive and false negative are likewise not used in this study, and also, precision will not be split in sensitivity (for false positives) and specificity (for negatives). As neither greece nor rome are inherently positive nor negative, such concepts have no place in our project (unless a build-in function automatically displays them).\n",
    "\n",
    "As far as right/wrong predictions are concerned we will use adapted precision metrics we will call \"rome recall\" and \"greece recall\". As the name implies, they describe to total rome( greece) outcomes correctly predicted out of the entire number of rome(greece) outcomes in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f02bcae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-19T14:13:19.879738Z",
     "start_time": "2022-01-19T14:13:19.359768Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287da70d",
   "metadata": {},
   "source": [
    "- the real scraping was initially done for 3000 posts for each subreddit, 6000 in total, using the cell below. the results were saved and were used throughout the project. the cell now scrapes only 500 posts each, to keep it integrated while saving time. the new results are not used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a851aa2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-19T14:15:39.361985Z",
     "start_time": "2022-01-19T14:13:19.879738Z"
    }
   },
   "outputs": [],
   "source": [
    "url = 'https://api.pushshift.io/reddit/search/submission'  # the base link\n",
    "\n",
    "reddit = None  # instantiate a new Ancient Greece/Rome dataframe,\n",
    "# set it to 'none' initially\n",
    "\n",
    "# we'll be scraping ancientgreece and ancientrome subreddits\n",
    "for subreddits in ['ancientgreece', 'ancientrome']:\n",
    "\n",
    "    utc_time = 1894262032  # set initial time to year 2030,\n",
    "#     so it starts scraping from the latest post\n",
    "\n",
    "# (loop 5 times) X (2 subreddits) X (100 posts per loop) = 1000 posts\n",
    "    for i in range(5): # the initial loop was set to 30, giving us\n",
    "        # 6000 posts to work with\n",
    "        params = {'subreddit': subreddits,\n",
    "                  'size': 100,\n",
    "                  'before': utc_time}\n",
    "\n",
    "        req = requests.get(url, params)\n",
    "        posts = req.json()['data']\n",
    "        if reddit is None:\n",
    "            # create a dataframe only on the first iteration\n",
    "            reddit = pd.DataFrame(\n",
    "                posts)[['created_utc', 'title', 'selftext', 'subreddit']]\n",
    "        else:\n",
    "            # on all subsequent iteration, concat by row\n",
    "            reddit_temp = pd.DataFrame(\n",
    "                posts)[['created_utc', 'title', 'selftext', 'subreddit']]\n",
    "            reddit = pd.concat([reddit, reddit_temp], ignore_index=True)\n",
    "\n",
    "        utc_time = reddit.iloc[-1, 0]  # resets time to the time of the \n",
    "        #last row so the next iteration scrapes only the posts that came \n",
    "        # earlier, resulting in no overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b449cd9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-19T14:15:39.407673Z",
     "start_time": "2022-01-19T14:15:39.361985Z"
    }
   },
   "outputs": [],
   "source": [
    "# this file is different everytime we run the notebook...\n",
    "reddit.to_csv('../datasets/ancients.csv', index=False)\n",
    "\n",
    "# ... while the file below is used for NLP -  it was saved once, code commented out, so it's always the same. \n",
    "# reddit.to_csv('../datasets/ancients_for_NLP.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
